{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import pickle\n",
    "from glob import glob\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "import nltk\n",
    "import spacy\n",
    "import logging\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import torchtext\n",
    "import torchtext.datasets as datasets\n",
    "from torchtext import data, vocab\n",
    "\n",
    "from IPython.display import display\n",
    "\n",
    "LOGGER = logging.getLogger(\"toxic_dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepare_csv(train_csv, test_csv, split=0.2, seed=999):\n",
    "    if not os.path.exists('data'):\n",
    "        os.mkdir('data')\n",
    "        \n",
    "    # read train csv file\n",
    "    df_train = pd.read_csv(train_csv)\n",
    "    df_train[\"comment_text\"] = df_train.comment_text.str.replace(\"\\n\", \" \")\n",
    "    \n",
    "    # create validation data\n",
    "    idx = np.arange(df_train.shape[0])\n",
    "    np.random.seed(seed)\n",
    "    np.random.shuffle(idx)\n",
    "    val_size = int(len(idx) * split)\n",
    "    df_train.iloc[idx[val_size:], :].to_csv(\"data/dataset_train.csv\", index=False)\n",
    "    df_train.iloc[idx[:val_size], :].to_csv(\"data/dataset_val.csv\", index=False)\n",
    "    \n",
    "    # read test csv file\n",
    "    df_test = pd.read_csv(test_csv)\n",
    "    df_test[\"comment_text\"] = df_test.comment_text.str.replace(\"\\n\", \" \")\n",
    "    df_test.to_csv(\"data/dataset_test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Toxic Comments Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_dir = 'D:/datasets/kaggle/toxic_comments'\n",
    "\n",
    "train_csv = f'{data_dir}/train.csv'\n",
    "test_csv = f'{data_dir}/test.csv'\n",
    "\n",
    "# batch_size = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['glove', 'jigsaw', 'sample_submission.csv', 'sample_submission.csv.zip', 'test.csv', 'test.csv.zip', 'train.csv', 'train.csv.zip']\n"
     ]
    }
   ],
   "source": [
    "print(os.listdir(data_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(train_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>24837</th>\n",
       "      <td>256828841699</td>\n",
       "      <td>to be unblocked, its just</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91552</th>\n",
       "      <td>956238397504</td>\n",
       "      <td>34, 21 July 2008 (UTC)</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15561</th>\n",
       "      <td>160882777477</td>\n",
       "      <td>Request on 14:26:36, 27 November 2014 for assi...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25797</th>\n",
       "      <td>267263413007</td>\n",
       "      <td>\"\\nI've changed it to \"\"9/11 Truth movement is...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90555</th>\n",
       "      <td>945944934292</td>\n",
       "      <td>\"  —Preceding unsigned comment added by 109.11...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text  toxic  \\\n",
       "24837  256828841699                          to be unblocked, its just      0   \n",
       "91552  956238397504                             34, 21 July 2008 (UTC)      0   \n",
       "15561  160882777477  Request on 14:26:36, 27 November 2014 for assi...      0   \n",
       "25797  267263413007  \"\\nI've changed it to \"\"9/11 Truth movement is...      0   \n",
       "90555  945944934292  \"  —Preceding unsigned comment added by 109.11...      0   \n",
       "\n",
       "       severe_toxic  obscene  threat  insult  identity_hate  \n",
       "24837             0        0       0       0              0  \n",
       "91552             0        0       0       0              0  \n",
       "15561             0        0       0       0              0  \n",
       "25797             0        0       0       0              0  \n",
       "90555             0        0       0       0              0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(train_df.sample(n=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sos_token = 0\n",
    "eos_token = 1\n",
    "\n",
    "class Vocabulary(object):\n",
    "    def __init__(self):\n",
    "        self.word2index = {\"<sos>\": 0, \"<eos>\": 1}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {}\n",
    "        self.count = 2\n",
    "    \n",
    "    def add_word(self, word):\n",
    "        if not word in self.word2index:\n",
    "            self.word2index[word] = self.count\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.count] = word\n",
    "            self.count += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1\n",
    "    \n",
    "    def add_sentence(self, sentence):\n",
    "        for word in sentence.split(\" \"):\n",
    "            self.add_word(word)\n",
    "            \n",
    "    def __len__(self):\n",
    "        return self.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "945ae7cfd28249ff972cff672b4e86f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in Jupyter Notebook or JupyterLab, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another notebook frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Building vocabulary', max=95851), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "vocab = Vocabulary()\n",
    "all_comments_text = train_df[\"comment_text\"]\n",
    "for text in tqdm_notebook(all_comments_text, desc='Building vocabulary'):\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    for sent in sentences:\n",
    "        vocab.add_sentence(sent)\n",
    "        \n",
    "with open('vocab.pkl', 'bw') as f:\n",
    "    pickle.dump(vocab, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab = pickle.load(open('vocab.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400126\n"
     ]
    }
   ],
   "source": [
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All: 95851\n",
      "toxic: 9237\n",
      "severe_toxic: 965\n",
      "obscene: 5109\n",
      "threat: 305\n",
      "insult: 4765\n",
      "identity_hate: 814\n"
     ]
    }
   ],
   "source": [
    "print(\"All:\", len(train_df))\n",
    "print(\"toxic:\", len(train_df[train_df['toxic'] == 1]))\n",
    "print(\"severe_toxic:\", len(train_df[train_df['severe_toxic'] == 1]))\n",
    "print(\"obscene:\", len(train_df[train_df['obscene'] == 1]))\n",
    "print(\"threat:\", len(train_df[train_df['threat'] == 1]))\n",
    "print(\"insult:\", len(train_df[train_df['insult'] == 1]))\n",
    "print(\"identity_hate:\", len(train_df[train_df['identity_hate'] == 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## spaCy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre>conda install -c conda-forge spacy</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NLP = spacy.load('en')\n",
    "MAX_CHARS = 20000\n",
    "\n",
    "def tokenizer(comment):\n",
    "    comment = re.sub(\n",
    "        r\"[\\*\\\"“”\\n\\\\…\\+\\-\\/\\=\\(\\)‘•:\\[\\]\\|’\\!;]\", \" \", \n",
    "        str(comment))\n",
    "    comment = re.sub(r\"[ ]+\", \" \", comment)\n",
    "    comment = re.sub(r\"\\!+\", \"!\", comment)\n",
    "    comment = re.sub(r\"\\,+\", \",\", comment)\n",
    "    comment = re.sub(r\"\\?+\", \"?\", comment)\n",
    "    if (len(comment) > MAX_CHARS):\n",
    "        comment = comment[:MAX_CHARS]\n",
    "    return [x.text for x in NLP.tokenizer(comment) if x.text != \" \"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_dataset(train_scv, test_csv, split=0.2, fix_length=100, lower=False, vectors=None):\n",
    "    if vectors is not None:\n",
    "        # pretrain vectors only supports all lower cases\n",
    "        lower = True\n",
    "    \n",
    "    LOGGER.debug(\"Preparing CSV files...\")\n",
    "#     prepare_csv(train_csv, test_csv, split)\n",
    "    \n",
    "    comment = data.Field(\n",
    "        sequential=True,\n",
    "        fix_length=fix_length,\n",
    "        tokenize=tokenizer,\n",
    "        pad_first=True,\n",
    "        tensor_type=torch.cuda.LongTensor,\n",
    "        lower=lower\n",
    "    )\n",
    "    \n",
    "    print(\"Reading train csv file...\")\n",
    "    train, val = data.TabularDataset.splits(\n",
    "        path='data/', format='csv', skip_header=True,\n",
    "        train='dataset_train.csv', validation='dataset_val.csv',\n",
    "        fields=[\n",
    "            ('id', None),\n",
    "            ('comment_text', comment),\n",
    "            ('toxic', data.Field(\n",
    "                use_vocab=False, sequential=False,\n",
    "                tensor_type=torch.cuda.ByteTensor)),\n",
    "            ('severe_toxic', data.Field(\n",
    "                use_vocab=False, sequential=False, \n",
    "                tensor_type=torch.cuda.ByteTensor)),\n",
    "            ('obscene', data.Field(\n",
    "                use_vocab=False, sequential=False, \n",
    "                tensor_type=torch.cuda.ByteTensor)),\n",
    "            ('threat', data.Field(\n",
    "                use_vocab=False, sequential=False, \n",
    "                tensor_type=torch.cuda.ByteTensor)),\n",
    "            ('insult', data.Field(\n",
    "                use_vocab=False, sequential=False, \n",
    "                tensor_type=torch.cuda.ByteTensor)),\n",
    "            ('identity_hate', data.Field(\n",
    "                use_vocab=False, sequential=False, \n",
    "                tensor_type=torch.cuda.ByteTensor)),\n",
    "        ])\n",
    "    \n",
    "    print(\"Reading test csv file...\")\n",
    "    test = data.TabularDataset(\n",
    "        path='data/dataset_test.csv', format='csv', \n",
    "        skip_header=True,\n",
    "        fields=[\n",
    "            ('id', None),\n",
    "            ('comment_text', comment)\n",
    "        ])\n",
    "    \n",
    "    print(\"Building vocabulary...\")\n",
    "    comment.build_vocab(\n",
    "        train, val, test,\n",
    "        max_size=20000,\n",
    "        min_freq=50,\n",
    "        vectors=vectors\n",
    "    )\n",
    "    \n",
    "    print(\"Done preparing the datasets\")\n",
    "    return train, val, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading train csv file...\n",
      "Reading test csv file...\n",
      "Building vocabulary...\n",
      "Done preparing the datasets\n",
      "Wall time: 8min 2s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_ds, valid_ds, test_ds = get_dataset(train_csv, test_csv, split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76681\n",
      "19170\n",
      "153164\n"
     ]
    }
   ],
   "source": [
    "print(len(train_ds.examples))\n",
    "print(len(valid_ds.examples))\n",
    "print(len(test_ds.examples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'comment_text': <torchtext.data.field.Field at 0x2152f039eb8>,\n",
       " 'id': None,\n",
       " 'identity_hate': <torchtext.data.field.Field at 0x2152f039a90>,\n",
       " 'insult': <torchtext.data.field.Field at 0x2152f039b70>,\n",
       " 'obscene': <torchtext.data.field.Field at 0x2152f039e48>,\n",
       " 'severe_toxic': <torchtext.data.field.Field at 0x2152f039c88>,\n",
       " 'threat': <torchtext.data.field.Field at 0x2152f039b00>,\n",
       " 'toxic': <torchtext.data.field.Field at 0x2152f039ac8>}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds.fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(len(train_ds.examples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_iterator(dataset, batch_size, train=True, \n",
    "    shuffle=True, repeat=False):\n",
    "    dataset_iter = data.Iterator(\n",
    "        dataset, batch_size=batch_size, device=0,\n",
    "        train=train, shuffle=shuffle, repeat=repeat,\n",
    "        sort=False\n",
    "    )\n",
    "    return dataset_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "train_iter = get_iterator(train_ds, batch_size, train=True, shuffle=True, repeat=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      "    1\n",
      "    1\n",
      "    1\n",
      "    1\n",
      "    1\n",
      "    1\n",
      "    1\n",
      "    1\n",
      "    1\n",
      "    1\n",
      "    1\n",
      "    1\n",
      "    1\n",
      "    1\n",
      "    1\n",
      "    1\n",
      "    1\n",
      "    1\n",
      "    1\n",
      "    1\n",
      "    1\n",
      "    1\n",
      "    1\n",
      "    1\n",
      "    1\n",
      "    1\n",
      "    1\n",
      "    1\n",
      "    1\n",
      "    1\n",
      "    1\n",
      "    1\n",
      "    1\n",
      "    1\n",
      "    1\n",
      "    1\n",
      "    1\n",
      "    1\n",
      "    1\n",
      "    1\n",
      "    1\n",
      "    1\n",
      "    1\n",
      "    1\n",
      "    1\n",
      "    1\n",
      "    1\n",
      "    1\n",
      "    1\n",
      "    1\n",
      "    1\n",
      "    1\n",
      "    1\n",
      "    1\n",
      "    1\n",
      "    1\n",
      "    1\n",
      "    1\n",
      "    1\n",
      "    1\n",
      "    1\n",
      "    1\n",
      "    1\n",
      "    1\n",
      "    1\n",
      "    1\n",
      "    1\n",
      "    1\n",
      "    1\n",
      "    1\n",
      "    1\n",
      "    1\n",
      "    1\n",
      "    1\n",
      "    1\n",
      "    1\n",
      "    1\n",
      "    1\n",
      "    1\n",
      "    1\n",
      "    1\n",
      "    1\n",
      "    1\n",
      "    1\n",
      "    1\n",
      "    1\n",
      "    1\n",
      "    1\n",
      "    1\n",
      "    1\n",
      "    1\n",
      "    1\n",
      "    1\n",
      "   83\n",
      "  187\n",
      "   10\n",
      " 4613\n",
      "    0\n",
      "  932\n",
      "    2\n",
      "[torch.cuda.LongTensor of size 100x1 (GPU 0)]\n",
      "\n",
      "Variable containing:\n",
      " 0  0  0  0  0  0\n",
      "[torch.cuda.ByteTensor of size 1x6 (GPU 0)]\n",
      "\n",
      "Variable containing:\n",
      "     1\n",
      "     1\n",
      "     1\n",
      "     1\n",
      "     1\n",
      "     1\n",
      "     1\n",
      "     1\n",
      "     1\n",
      "     1\n",
      "     1\n",
      "     1\n",
      "     1\n",
      "     1\n",
      "     1\n",
      "     1\n",
      "     1\n",
      "     1\n",
      "     1\n",
      "     1\n",
      "     1\n",
      "   122\n",
      "    10\n",
      "    34\n",
      "  1512\n",
      "  4203\n",
      "     2\n",
      "   154\n",
      "    20\n",
      "   151\n",
      "  1650\n",
      "  1099\n",
      "  4404\n",
      "   600\n",
      "    13\n",
      "     0\n",
      "     9\n",
      "  1470\n",
      "    23\n",
      "   136\n",
      "     2\n",
      "     0\n",
      "   131\n",
      "     5\n",
      "   145\n",
      "   409\n",
      "     6\n",
      " 12452\n",
      "     2\n",
      "     0\n",
      "    27\n",
      "    16\n",
      "   131\n",
      "     5\n",
      "  1071\n",
      "   101\n",
      "   587\n",
      "     2\n",
      "     7\n",
      "    70\n",
      "    30\n",
      "   262\n",
      "    39\n",
      "     4\n",
      "  9445\n",
      "    20\n",
      "   143\n",
      "   204\n",
      "     8\n",
      "  1167\n",
      "  1316\n",
      "     2\n",
      " 13068\n",
      "  1623\n",
      "    20\n",
      "  1744\n",
      "    85\n",
      "    55\n",
      "    27\n",
      "    16\n",
      "   401\n",
      "     8\n",
      "   640\n",
      "  1561\n",
      "    10\n",
      "  7240\n",
      "     2\n",
      "     7\n",
      "   179\n",
      "  1399\n",
      "     5\n",
      "   638\n",
      "   926\n",
      "   808\n",
      "  1199\n",
      "   433\n",
      "     2\n",
      "   173\n",
      "    11\n",
      "     2\n",
      "[torch.cuda.LongTensor of size 100x1 (GPU 0)]\n",
      "\n",
      "Variable containing:\n",
      " 0  0  0  0  0  0\n",
      "[torch.cuda.ByteTensor of size 1x6 (GPU 0)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, examples in enumerate(train_iter):\n",
    "    x = examples.comment_text # (fix_length, batch_size) Tensor\n",
    "    y = torch.stack([\n",
    "        examples.toxic, \n",
    "        examples.severe_toxic, \n",
    "        examples.obscene,\n",
    "        examples.threat, \n",
    "        examples.insult, \n",
    "        examples.identity_hate\n",
    "    ], dim=1)\n",
    "    \n",
    "    print(x)\n",
    "    print(y)\n",
    "    if i >= 1: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading pre-trained word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model.word_em.weight.data = train_dataset.fields[\"comment_text\"].vocab.vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder RNN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "use_gpu = torch.cuda.is_available()\n",
    "\n",
    "def to_var(x, volatile=False):\n",
    "    x = Variable(x, volatile=volatile)\n",
    "    return x.cuda() if use_gpu else x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes=6, num_layers=1):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, num_classes)\n",
    "    \n",
    "    def forward(self, x, hidden):\n",
    "        output = self.embedding(x)\n",
    "        for i in range(self.num_layers):\n",
    "            output, hidden = self.gru(output, hidden)\n",
    "        output = self.out(hidden)\n",
    "        output = F.relu(output)\n",
    "        output = F.dropout(output, p=0.1)\n",
    "        output = F.sigmoid(output)\n",
    "        return output\n",
    "    \n",
    "    def init_hidden(self):\n",
    "        return to_var(torch.zeros((1, 1, self.hidden_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14636\n"
     ]
    }
   ],
   "source": [
    "hidden_size = 128\n",
    "num_layers = 1\n",
    "vocab = train_ds.fields['comment_text'].vocab\n",
    "print(len(vocab))\n",
    "model = EncoderRNN(len(vocab), hidden_size, num_classes=6, num_layers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if use_gpu:\n",
    "    model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EncoderRNN(\n",
       "  (embedding): Embedding(14636, 128)\n",
       "  (gru): GRU(128, 128)\n",
       "  (out): Linear(in_features=128, out_features=6, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "criterion = nn.BCELoss()\n",
    "if use_gpu:\n",
    "    criterion = criterion.cuda()\n",
    "    \n",
    "optimizer = optim.Adam(rnn.parameters(), lr=0.002)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8138cca4529b4220bd4837551e348493",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in Jupyter Notebook or JupyterLab, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another notebook frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " loss = 0.69315\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-17:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Razavi\\Anaconda3\\lib\\threading.py\", line 916, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"C:\\Users\\Razavi\\Anaconda3\\lib\\site-packages\\tqdm\\_tqdm.py\", line 144, in run\n",
      "    for instance in self.tqdm_cls._instances:\n",
      "  File \"C:\\Users\\Razavi\\Anaconda3\\lib\\_weakrefset.py\", line 60, in __iter__\n",
      "    for itemref in self.data:\n",
      "RuntimeError: Set changed size during iteration\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " loss = 0.69315\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-114-a080a9368d05>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[1;31m# backward step\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\autograd\\variable.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[0;32m    165\u001b[0m                 \u001b[0mVariable\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    166\u001b[0m         \"\"\"\n\u001b[1;32m--> 167\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_variables\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    168\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    169\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(variables, grad_variables, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[0;32m     97\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m---> 99\u001b[1;33m         variables, grad_variables, retain_graph)\n\u001b[0m\u001b[0;32m    100\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_epochs = 1\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0.0\n",
    "#     h = to_var(torch.zeros((num_layers, batch_size, hidden_size)))\n",
    "    h = rnn.init_hidden()\n",
    "    \n",
    "    for i, examples in tqdm_notebook(enumerate(train_iter)):\n",
    "        x = examples.comment_text\n",
    "        y = torch.stack([examples.toxic, examples.severe_toxic, examples.obscene,\n",
    "                         examples.threat, examples.insult, examples.identity_hate], dim=1)\n",
    "        \n",
    "        # forward step\n",
    "        output = rnn(x, h)\n",
    "        \n",
    "        # loss\n",
    "        loss = criterion(output, y.float().view(1, 1, -1))\n",
    "        \n",
    "        # backward step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # stats\n",
    "        sys.stdout.flush()\n",
    "        sys.stdout.write('\\r loss = {:.5f}'.format(loss.data[0]))\n",
    "        \n",
    "        if i > len(train_ds.examples): break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_epochs = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Corpus(object):\n",
    "    def __init__(self, DATA_DIR, filenames):\n",
    "        self.vocab = Vocabulary()\n",
    "        self.data = self.tokenize(DATA_DIR, filenames)\n",
    "\n",
    "    def tokenize(self, DATA_DIR, filenames):\n",
    "        for filename in filenames:\n",
    "            path = os.path.join(DATA_DIR, filename)\n",
    "            with open(path, 'r') as f:\n",
    "                tokens = 0\n",
    "                for line in f:\n",
    "                    words = line.split() + ['<eos>']\n",
    "                    tokens += len(words)\n",
    "                    for word in words:\n",
    "                        self.vocab.add_word(word)\n",
    "\n",
    "            # Tokenize file content\n",
    "            with open(path, 'r') as f:\n",
    "                ids = torch.LongTensor(tokens)\n",
    "                token = 0\n",
    "                for line in f:\n",
    "                    words = line.split() + ['<eos>']\n",
    "                    for word in words:\n",
    "                        ids[token] = self.dictionary.word2idx[word]\n",
    "                        token += 1\n",
    "\n",
    "        return ids\n",
    "\n",
    "class TxtDatasetProcessing(Dataset):\n",
    "    def __init__(self, data_path, txt_path, txt_filename, label_filename, sen_len, corpus):\n",
    "        self.txt_path = os.path.join(data_path, txt_path)\n",
    "        # reading txt file from file\n",
    "        txt_filepath = os.path.join(data_path, txt_filename)\n",
    "        fp = open(txt_filepath, 'r')\n",
    "        self.txt_filename = [x.strip() for x in fp]\n",
    "        fp.close()\n",
    "        # reading labels from file\n",
    "        label_filepath = os.path.join(data_path, label_filename)\n",
    "        fp_label = open(label_filepath, 'r')\n",
    "        labels = [int(x.strip()) for x in fp_label]\n",
    "        fp_label.close()\n",
    "        self.label = labels\n",
    "        self.corpus = corpus\n",
    "        self.sen_len = sen_len\n",
    "\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        filename = os.path.join(self.txt_path, self.txt_filename[index])\n",
    "        fp = open(filename, 'r')\n",
    "        txt = torch.LongTensor(np.zeros(self.sen_len, dtype=np.int64))\n",
    "        count = 0\n",
    "        clip = False\n",
    "        for words in fp:\n",
    "            for word in words.split():\n",
    "                if word.strip() in self.corpus.dictionary.word2idx:\n",
    "                    if count > self.sen_len - 1:\n",
    "                        clip = True\n",
    "                        break\n",
    "                    txt[count] = self.corpus.dictionary.word2idx[word.strip()]\n",
    "                    count += 1\n",
    "            if clip: break\n",
    "        label = torch.LongTensor([self.label[index]])\n",
    "        return txt, label\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.txt_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LSTMClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, label_size, batch_size):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n",
    "        self.hidden2label = nn.Linear(hidden_dim, label_size)\n",
    "        self.hidden = self.init_hidden()\n",
    "\n",
    "    def init_hidden(self):\n",
    "        h0 = to_var(torch.zeros(1, self.batch_size, self.hidden_dim))\n",
    "        c0 = to_var(torch.zeros(1, self.batch_size, self.hidden_dim))\n",
    "        return h0, c0\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        embeds = self.embeddings(sentence)\n",
    "        x = embeds.view(len(sentence), self.batch_size, -1)\n",
    "        output, self.hidden = self.lstm(x, self.hidden)\n",
    "        y = self.hidden2label(output[-1])\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## parameter setting\n",
    "epochs = 50\n",
    "batch_size = 5\n",
    "learning_rate = 0.002\n",
    "\n",
    "embedding_dim = 100\n",
    "hidden_dim = 50\n",
    "seq_len = 100\n",
    "num_classes = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train dataset\n",
    "\n",
    "# test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = LSTMClassifier(embedding_dim=embedding_dim, hidden_dim=hidden_dim, \n",
    "                       vocab_size=len(vocab), label_size=num_classes, \n",
    "                       batch_size=batch_size)\n",
    "\n",
    "if use_gpu:\n",
    "    model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "criterion = nn.BCELoss()\n",
    "if use_gpu:\n",
    "    criterion = criterion.cuda()\n",
    "    \n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_loss_ = []\n",
    "test_loss_ = []\n",
    "train_acc_ = []\n",
    "test_acc_ = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "#     optimizer = adjust_learning_rate(optimizer, epoch)\n",
    "\n",
    "    ## training epoch\n",
    "    total_acc = 0.0\n",
    "    total_loss = 0.0\n",
    "    total = 0.0\n",
    "    \n",
    "    for i, traindata in enumerate(train_loader):\n",
    "        train_inputs, train_labels = traindata\n",
    "        train_labels = torch.squeeze(train_labels)\n",
    "\n",
    "        if use_gpu:\n",
    "            train_inputs, train_labels = Variable(train_inputs.cuda()), train_labels.cuda()\n",
    "        else: train_inputs = Variable(train_inputs)\n",
    "\n",
    "        model.zero_grad()\n",
    "        model.batch_size = len(train_labels)\n",
    "        model.hidden = model.init_hidden()\n",
    "        output = model(train_inputs.t())\n",
    "\n",
    "        loss = loss_function(output, Variable(train_labels))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # calc training acc\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "        total_acc += (predicted == train_labels).sum()\n",
    "        total += len(train_labels)\n",
    "        total_loss += loss.data[0]\n",
    "\n",
    "    train_loss_.append(total_loss / total)\n",
    "    train_acc_.append(total_acc / total)\n",
    "    ## testing epoch\n",
    "    total_acc = 0.0\n",
    "    total_loss = 0.0\n",
    "    total = 0.0\n",
    "    for iter, testdata in enumerate(test_loader):\n",
    "        test_inputs, test_labels = testdata\n",
    "        test_labels = torch.squeeze(test_labels)\n",
    "\n",
    "        if use_gpu:\n",
    "            test_inputs, test_labels = Variable(test_inputs.cuda()), test_labels.cuda()\n",
    "        else: test_inputs = Variable(test_inputs)\n",
    "\n",
    "        model.batch_size = len(test_labels)\n",
    "        model.hidden = model.init_hidden()\n",
    "        output = model(test_inputs.t())\n",
    "\n",
    "        loss = loss_function(output, Variable(test_labels))\n",
    "\n",
    "        # calc testing acc\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "        total_acc += (predicted == test_labels).sum()\n",
    "        total += len(test_labels)\n",
    "        total_loss += loss.data[0]\n",
    "    test_loss_.append(total_loss / total)\n",
    "    test_acc_.append(total_acc / total)\n",
    "\n",
    "    print('[Epoch: %3d/%3d] Training Loss: %.3f, Testing Loss: %.3f, Training Acc: %.3f, Testing Acc: %.3f'\n",
    "          % (epoch, epochs, train_loss_[epoch], test_loss_[epoch], train_acc_[epoch], test_acc_[epoch]))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
