{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <h1 align=\"center\">Lesson 5: Text Classification - Sentiment Analysis</h1>\n",
    "    <h3 align=\"center\"><a href=\"http://www.snrazavi.ir\">Seyed Naser RAZAVI</a></h3>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h6>Text Classification:</h6> Assigning a label (from a set of pre-defined labels) to a given text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h6>Some applications:</h6>\n",
    "\n",
    "- **Sentiment Analysis:**  Determining the polarity of a text (`positive` or `negative`).\n",
    "\n",
    "- **Spam detection:** email, web pages, etc.\n",
    "\n",
    "- **Toxic text detection:** insult, racism, hatred, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/razavi/Documents/mygithub/DeepLearning/Deep_Learning_in_Python_2018/venv/lib/python3.6/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: Unexpected error from cudaGetDeviceCount(). Did you run some cuda functions before calling NumCudaDevices() that might have already set an error? Error 803: system has unsupported display driver / cuda driver combination (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:115.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import spacy\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "from glob import glob\n",
    "import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from utils import *\n",
    "from data_utils import Vocabulary, tokenizer\n",
    "from train_utils import train\n",
    "\n",
    "\n",
    "# setup\n",
    "NLP = spacy.load('en_core_web_sm')  # NLP toolkit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "Bromwell High is a cartoon comedy. \n",
    "It ran at the same time as some other programs about school life, such as 'Teachers'. \n",
    "My 35 years in the teaching profession lead me to believe that Bromwell High's \n",
    "satire is much closer to reality than is 'Teachers'. \n",
    "The scramble to survive financially, the insightful students who can see \n",
    "right through their pathetic teachers' pomp, the pettiness of the whole situation, \n",
    "all remind me of the schools I knew and their students. \n",
    "When I saw the episode in which a student repeatedly tried to burn down the school, \n",
    "I immediately recalled ......... at .......... High. \n",
    "A classic line: INSPECTOR: I'm here to sack one of your teachers. \n",
    "STUDENT: Welcome to Bromwell High. \n",
    "I expect that many adults of my age think that Bromwell High is far fetched. \n",
    "What a pity that it isn't!!!\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Bromwell High is a cartoon comedy.  It ran at the same time as some other programs about school life, such as 'Teachers'.  My 35 years in the teaching profession lead me to believe that Bromwell High's  satire is much closer to reality than is 'Teachers'.  The scramble to survive financially, the insightful students who can see  right through their pathetic teachers' pomp, the pettiness of the whole situation,  all remind me of the schools I knew and their students.  When I saw the episode in which a student repeatedly tried to burn down the school,  I immediately recalled ......... at .......... High.  A classic line  INSPECTOR  I'm here to sack one of your teachers.  STUDENT  Welcome to Bromwell High.  I expect that many adults of my age think that Bromwell High is far fetched.  What a pity that it isn't!!! \n"
     ]
    }
   ],
   "source": [
    "text = re.sub(r\"[\\*\\\"“”\\n\\\\…\\+\\-\\/\\=\\(\\)‘•:\\[\\]\\|’;]\", \" \", str(text))\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Bromwell High is a cartoon comedy. It ran at the same time as some other programs about school life, such as 'Teachers'. My 35 years in the teaching profession lead me to believe that Bromwell High's satire is much closer to reality than is 'Teachers'. The scramble to survive financially, the insightful students who can see right through their pathetic teachers' pomp, the pettiness of the whole situation, all remind me of the schools I knew and their students. When I saw the episode in which a student repeatedly tried to burn down the school, I immediately recalled ......... at .......... High. A classic line INSPECTOR I'm here to sack one of your teachers. STUDENT Welcome to Bromwell High. I expect that many adults of my age think that Bromwell High is far fetched. What a pity that it isn't!!! \n"
     ]
    }
   ],
   "source": [
    "text = re.sub(r\"[ ]+\", \" \", text)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Bromwell High is a cartoon comedy. It ran at the same time as some other programs about school life, such as 'Teachers'. My 35 years in the teaching profession lead me to believe that Bromwell High's satire is much closer to reality than is 'Teachers'. The scramble to survive financially, the insightful students who can see right through their pathetic teachers' pomp, the pettiness of the whole situation, all remind me of the schools I knew and their students. When I saw the episode in which a student repeatedly tried to burn down the school, I immediately recalled ......... at .......... High. A classic line INSPECTOR I'm here to sack one of your teachers. STUDENT Welcome to Bromwell High. I expect that many adults of my age think that Bromwell High is far fetched. What a pity that it isn't! \n"
     ]
    }
   ],
   "source": [
    "text = re.sub(r\"\\!+\", \"!\", text)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' ', 'Bromwell', 'High', 'is', 'a', 'cartoon', 'comedy', '.', 'It', 'ran', 'at', 'the', 'same', 'time', 'as', 'some', 'other', 'programs', 'about', 'school', 'life', ',', 'such', 'as', \"'\", 'Teachers', \"'\", '.', 'My', '35', 'years', 'in', 'the', 'teaching', 'profession', 'lead', 'me', 'to', 'believe', 'that', 'Bromwell', 'High', \"'s\", 'satire', 'is', 'much', 'closer', 'to', 'reality', 'than', 'is', \"'\", 'Teachers', \"'\", '.', 'The', 'scramble', 'to', 'survive', 'financially', ',', 'the', 'insightful', 'students', 'who', 'can', 'see', 'right', 'through', 'their', 'pathetic', 'teachers', \"'\", 'pomp', ',', 'the', 'pettiness', 'of', 'the', 'whole', 'situation', ',', 'all', 'remind', 'me', 'of', 'the', 'schools', 'I', 'knew', 'and', 'their', 'students', '.', 'When', 'I', 'saw', 'the', 'episode', 'in', 'which', 'a', 'student', 'repeatedly', 'tried', 'to', 'burn', 'down', 'the', 'school', ',', 'I', 'immediately', 'recalled', '.........', 'at', '..........', 'High', '.', 'A', 'classic', 'line', 'INSPECTOR', 'I', \"'m\", 'here', 'to', 'sack', 'one', 'of', 'your', 'teachers', '.', 'STUDENT', 'Welcome', 'to', 'Bromwell', 'High', '.', 'I', 'expect', 'that', 'many', 'adults', 'of', 'my', 'age', 'think', 'that', 'Bromwell', 'High', 'is', 'far', 'fetched', '.', 'What', 'a', 'pity', 'that', 'it', 'is', \"n't\", '!']\n"
     ]
    }
   ],
   "source": [
    "tokens = [w.text for w in NLP.tokenizer(text)]\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizer and Vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have defined a function in `utils.py`, which gets the inputs text and splits it to a sequence of tokens. We have used **SpaCy** toolkit for tokeniztion and you need to install it to run the codes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "```python\n",
    "def tokenizer(text):\n",
    "    text = re.sub(r\"[\\*\\\"“”\\n\\\\…\\+\\-\\/\\=\\(\\)‘•:\\[\\]\\|’;]\", \" \", str(text))\n",
    "    text = re.sub(r\"[ ]+\", \" \", text)\n",
    "    text = re.sub(r\"\\!+\", \"!\", text)\n",
    "    text = re.sub(r\"\\,+\", \",\", text)\n",
    "    text = re.sub(r\"\\?+\", \"?\", text)\n",
    "    return [x.text for x in NLP.tokenizer(text) if x.text != \" \"]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install SpaCy "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Installation:\n",
    "<pre>conda install -c conda-forge spacy</pre>\n",
    "\n",
    "Download a language:\n",
    "<pre>python -m spacy download en_core_web_sm</pre>\n",
    "\n",
    "Usage:\n",
    "```python\n",
    "import spacy\n",
    "NLP = spacy.load('en_core_web_sm')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[IMDB](http://ai.stanford.edu/~amaas/data/sentiment/) Dataset\n",
    "- A dataset for binary sentiment classification.\n",
    "- It provides a set of 25,000 highly polar movie reviews for training, and 25,000 for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/media/razavi/DATA/datasets/aclImdb'\n",
    "\n",
    "vocab_path = 'vocab.pkl'\n",
    "\n",
    "# parameters\n",
    "max_len = 200\n",
    "min_count = 10\n",
    "batch_size = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dev', 'test', 'train']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['neg', 'pos']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(f'{data_dir}/train')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "654259110e1140669391a593fcf12380",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min length = 4\n",
      "Max length = 2470\n",
      "Mean = 231.15\n",
      "Std  = 171.32\n",
      "mean + 2 * sigma = 573.80\n"
     ]
    }
   ],
   "source": [
    "all_filenames = glob(f'{data_dir}/*/*/*.txt')\n",
    "num_words = [len(open(f).read().split(' ')) for f in tqdm.notebook.tqdm(all_filenames)]\n",
    "\n",
    "# print statistics\n",
    "print('Min length =', min(num_words))\n",
    "print('Max length =', max(num_words))\n",
    "\n",
    "print('Mean = {:.2f}'.format(np.mean(num_words)))\n",
    "print('Std  = {:.2f}'.format(np.std(num_words)))\n",
    "\n",
    "print('mean + 2 * sigma = {:.2f}'.format(np.mean(num_words) + 2.0 * np.std(num_words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD = '<pad>'  # special symbol we use for padding text\n",
    "UNK = '<unk>'  # special symbol we use for rare or unknown word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextClassificationDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, path, tokenizer, \n",
    "                 split='train', \n",
    "                 vocab_path='vocab.pkl', \n",
    "                 max_len=100, min_count=10):\n",
    "        \n",
    "        self.path = path\n",
    "        assert split in ['train', 'test']\n",
    "        self.split = split\n",
    "        self.vocab_path = vocab_path\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.min_count = min_count\n",
    "        \n",
    "        self.cache = {}\n",
    "        self.vocab = None\n",
    "        \n",
    "        self.classes = []\n",
    "        self.class_to_index = {}\n",
    "        self.text_files = []\n",
    "        \n",
    "        split_path = f'{path}/{split}'\n",
    "        \n",
    "        for cls_idx, label in enumerate(os.listdir(split_path)):\n",
    "            text_files = [(fname, cls_idx) for fname in glob(f'{split_path}/{label}/*.txt')]\n",
    "            self.text_files += text_files\n",
    "            self.classes += [label]\n",
    "            self.class_to_index[label] = cls_idx\n",
    "        \n",
    "        self.num_classes = len(self.classes)\n",
    "            \n",
    "        # build vocabulary from training and validation texts\n",
    "        self.build_vocab()\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        # read the tokenized text file and its label (neg=0, pos=1)\n",
    "        fname, class_idx = self.text_files[index]\n",
    "        \n",
    "        if fname in self.cache:\n",
    "            return self.cache[fname], class_idx\n",
    "        \n",
    "        # read text file \n",
    "        text = open(fname).read()\n",
    "        \n",
    "        # tokenize the text file\n",
    "        tokens = self.tokenizer(text.lower().strip())\n",
    "        \n",
    "        # padding and trimming\n",
    "        if len(tokens) < self.max_len:\n",
    "            num_pads = self.max_len - len(tokens)\n",
    "            tokens = [PAD] * num_pads + tokens\n",
    "        elif len(tokens) > self.max_len:\n",
    "            tokens = tokens[:self.max_len]\n",
    "            \n",
    "        # numericalizing\n",
    "        ids = torch.LongTensor(self.max_len)\n",
    "        for i, word in enumerate(tokens):\n",
    "            if word not in self.vocab.word2index:\n",
    "                ids[i] = self.vocab.word2index[UNK]  # unknown words\n",
    "            elif word != PAD and self.vocab.word2count[word] < self.min_count:\n",
    "                ids[i] = self.vocab.word2index[UNK]  # rare words\n",
    "            else:\n",
    "                ids[i] = self.vocab.word2index[word]\n",
    "                \n",
    "        # save in cache for future use\n",
    "        self.cache[fname] = ids\n",
    "        \n",
    "        return ids, class_idx\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.text_files)\n",
    "    \n",
    "    def build_vocab(self):\n",
    "        if not os.path.exists(self.vocab_path):\n",
    "            vocab = Vocabulary(self.tokenizer)\n",
    "            filenames = glob(f'{self.path}/*/*/*.txt')\n",
    "            for filename in tqdm.notebook.tqdm(filenames, desc='Building Vocab'):\n",
    "                with open(filename, encoding='utf8') as f:\n",
    "                    for line in f:\n",
    "                        vocab.add_sentence(line.lower())\n",
    "\n",
    "            # sort words by their frequencies\n",
    "            words = [(0, PAD), (0, UNK)]\n",
    "            words += sorted([(c, w) for w, c in vocab.word2count.items()], reverse=True)\n",
    "\n",
    "            self.vocab = Vocabulary(self.tokenizer)\n",
    "            for i, (count, word) in enumerate(words):\n",
    "                self.vocab.word2index[word] = i\n",
    "                self.vocab.word2count[word] = count\n",
    "                self.vocab.index2word[i] = word\n",
    "                self.vocab.count += 1\n",
    "\n",
    "            pickle.dump(self.vocab, open(self.vocab_path, 'wb'))\n",
    "        else:\n",
    "            self.vocab = pickle.load(open(self.vocab_path, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_ds = TextClassificationDataset(data_dir, tokenizer, 'train', vocab_path, max_len, min_count)\n",
    "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "valid_ds = TextClassificationDataset(data_dir, tokenizer, 'test', vocab_path, max_len, min_count)\n",
    "valid_dl = DataLoader(valid_ds, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25000"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25000"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(valid_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['neg', 'pos']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neg': 0, 'pos': 1}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds.class_to_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neg\n",
      "[    0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0    76     7     6   134    41    55  7587\n",
      "  1409    21     6  5058     4   539    53    22     6   615   138    15\n",
      "     9     6  1333   476     7  1910   211     4     6 10995  6496   316\n",
      "     9   655    96    40  2019     3  1119  2731    39     2   940     1\n",
      "     7    11    16  5607     4   483    11  2834  1910     2   226    69\n",
      "    22    66   809  1355   855   239    11    48   108   133  1485     4\n",
      "    68   153    43     2  1032   143    34   655   133     4     2 12829\n",
      "   421    64   105  1771   313   762     8     6   867 13190     4    27\n",
      "     6  1872   671    11    16   136    85    26   235   112    22    57\n",
      "    58   631    39   744    94     1     1     4   744   417  3054 15779\n",
      "     5 13196  8475    70    34   119  3217     4]\n"
     ]
    }
   ],
   "source": [
    "ids, label = train_ds[0]\n",
    "\n",
    "print(train_ds.classes[label])\n",
    "print(ids.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> story of a man who has unnatural feelings for a pig . starts out with a opening scene that is a terrific example of absurd comedy . a formal orchestra audience is turned into an insane , violent mob by the crazy <unk> of it 's singers . unfortunately it stays absurd the whole time with no general narrative eventually making it just too off putting . even those from the era should be turned off . the cryptic dialogue would make shakespeare seem easy to a third grader . on a technical level it 's better than you might think with some good cinematography by future great <unk> <unk> . future stars sally kirkland and frederic forrest can be seen briefly .\n"
     ]
    }
   ],
   "source": [
    "# convert back the sequence of integers into original text\n",
    "print(' '.join([train_ds.vocab.index2word[i.item()] for i in ids]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Story of a man who has unnatural feelings for a pig. Starts out with a opening scene that is a terrific example of absurd comedy. A formal orchestra audience is turned into an insane, violent mob by the crazy chantings of it's singers. Unfortunately it stays absurd the WHOLE time with no general narrative eventually making it just too off putting. Even those from the era should be turned off. The cryptic dialogue would make Shakespeare seem easy to a third grader. On a technical level it's better than you might think with some good cinematography by future great Vilmos Zsigmond. Future stars Sally Kirkland and Frederic Forrest can be seen briefly.\n"
     ]
    }
   ],
   "source": [
    "# print the original text\n",
    "print(open(train_ds.text_files[0][0]).read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vovcabulary size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size = 29506\n",
      "\n",
      "Most common words:\n",
      "the: 666713\n",
      ",: 543467\n",
      ".: 470130\n",
      "and: 324156\n",
      "a: 321800\n",
      "of: 289313\n",
      "to: 267961\n",
      "is: 217022\n",
      ">: 202243\n",
      "it: 187974\n"
     ]
    }
   ],
   "source": [
    "vocab = train_ds.vocab\n",
    "freqs = [(count, word) for (word, count) in vocab.word2count.items() if count >= min_count]\n",
    "vocab_size = len(freqs) + 2  # for PAD and UNK tokens\n",
    "print(f'Vocab size = {vocab_size}')\n",
    "\n",
    "print('\\nMost common words:')\n",
    "for c, w in sorted(freqs, reverse=True)[:10]:\n",
    "    print(f'{w}: {c}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Classifier with Attention mechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attention computes a weighted average of the hidden states of the LSTM Model.\n",
    "# In fact, it produce a weight for each hidden state at different time steps\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, 64),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, encoder_outputs):\n",
    "        # encoder_outputs = [batch size, sent len, hid dim]\n",
    "        energy = self.projection(encoder_outputs)\n",
    "        # energy = [batch size, sent len, 1]\n",
    "        weights = F.softmax(energy.squeeze(-1), dim=1)\n",
    "        # weights = [batch size, sent len]\n",
    "        outputs = (encoder_outputs * weights.unsqueeze(-1)).sum(dim=1)\n",
    "        # outputs = [batch size, hid dim]\n",
    "        return outputs, weights\n",
    "\n",
    "    \n",
    "class AttentionLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, bidirectional, dropout):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.embedding_dim = embed_size\n",
    "        self.num_layers = n_layers\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=n_layers,\n",
    "                            bidirectional=bidirectional, \n",
    "                            dropout= 0 if n_layers < 2 else dropout)\n",
    "        self.attention = SelfAttention(hidden_dim)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x = [sent len, batch size]\n",
    "        embedded = self.embedding(x)\n",
    "        # embedded = [sent len, batch size, emb dim]\n",
    "        output, (hidden, cell) = self.lstm(embedded)\n",
    "        # use 'batch_first' if you want batch size to be the 1st para\n",
    "        # output = [sent len, batch size, hid dim*num directions]\n",
    "        output = output[:, :, :self.hidden_dim] + output[:, :, self.hidden_dim:]\n",
    "        # output = [sent len, batch size, hid dim]\n",
    "        ouput = output.permute(1, 0, 2)\n",
    "        # ouput = [batch size, sent len, hid dim]\n",
    "        new_embed, weights = self.attention(ouput)\n",
    "        # new_embed = [batch size, hid dim]\n",
    "        # weights = [batch size, sent len]\n",
    "        new_embed = self.dropout(new_embed)\n",
    "        return self.fc(new_embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29506\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 2 + len([w for (w, c) in train_ds.vocab.word2count.items() if c >= min_count])\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM parameters\n",
    "embed_size = 100\n",
    "hidden_size = 256\n",
    "num_layers = 1\n",
    "\n",
    "# training parameters\n",
    "lr = 0.001\n",
    "num_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AttentionLSTM(vocab_size, embed_size, hidden_size, \n",
    "                      output_dim=train_ds.num_classes, \n",
    "                      n_layers=num_layers, bidirectional=True, dropout=0.5)\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "criterion = criterion.to(device)\n",
    "    \n",
    "optimizer = optim.Adam(model.parameters(), lr=lr, betas=(0.7, 0.99))\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch:  1/10] | Training Loss: 0.010 | Testing Loss: 0.008 | Training Acc: 75.25 | Testing Acc: 82.03\n",
      "[Epoch:  2/10] | Training Loss: 0.007 | Testing Loss: 0.007 | Training Acc: 85.78 | Testing Acc: 84.34\n",
      "[Epoch:  3/10] | Training Loss: 0.005 | Testing Loss: 0.008 | Training Acc: 89.56 | Testing Acc: 85.02\n",
      "[Epoch:  4/10] | Training Loss: 0.004 | Testing Loss: 0.007 | Training Acc: 92.84 | Testing Acc: 85.07\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20b84e4dae584c07a30087d389bd6010",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab84dcea780d4dceb8a09efcc31bb530",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "hist = train(model, train_dl, valid_dl, criterion, optimizer, device, scheduler, num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load a trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM parameters\n",
    "max_len = 400\n",
    "min_count = 10\n",
    "embed_size = 256\n",
    "hidden_size = 1024\n",
    "num_layers = 1\n",
    "epoch = 10\n",
    "\n",
    "\n",
    "model = LSTMClassifier(embed_size=embed_size, \n",
    "                       hidden_size=hidden_size, \n",
    "                       vocab_size=vocab_size,\n",
    "                       num_layers=num_layers,\n",
    "                       num_classes=train_ds.num_classes, \n",
    "                       batch_size=batch_size)\n",
    "\n",
    "filename = f'models/lstm-{num_layers}-{max_len}-{epoch}-{embed_size}-{hidden_size}.pth'\n",
    "\n",
    "try:\n",
    "    model.load_state_dict(torch.load())\n",
    "    model = model.to(device)\n",
    "except:\n",
    "    print('Load failed! The file does not exist.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing word vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='imgs/wordvecs_persian.png' width='80%'/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vecs = model.embedding.weight.data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(word_vecs.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(word_vecs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improvements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Fine-tuning hyper-parameters\n",
    "- Bidirectional LSTM\n",
    "- Pre-trained word vectors ([GloVe](https://nlp.stanford.edu/projects/glove/), [FastText](https://code.facebook.com/posts/1438652669495149/fair-open-sources-fasttext/), etc.)\n",
    "- Dropouts and regularization (https://arxiv.org/pdf/1708.02182.pdf)\n",
    "- Attention mechanism (https://distill.pub/2016/augmented-rnns/)\n",
    "- Use GRU or QRNN (https://arxiv.org/pdf/1611.01576.pdf)\n",
    "- Pre-training with a language model (https://arxiv.org/pdf/1605.07725.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention mechanism"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='imgs/attention-example.png' width='90%'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QRNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='imgs/QRNN.png' width='100%'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Programming Excersize: Toxic Texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Try to get an accuracy equal (or above 90) % for IMDB dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further Study"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h6>LSTM</h6> \n",
    "- http://colah.github.io/posts/2015-08-Understanding-LSTMs/\n",
    "- https://medium.com/mlreview/understanding-lstm-and-its-diagrams-37e2f46f1714"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h6>Word Vectors and NLP</h6>\n",
    "- https://einstein.ai/research/learned-in-translation-contextualized-word-vectors"
   ]
  }
 ],
 "metadata": {
  "gist": {
   "data": {
    "description": "Untitled.ipynb",
    "public": false
   },
   "id": ""
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
